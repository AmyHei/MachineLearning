{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7982bad55cd54d575ce8f215852d463c81a8dd8f"
   },
   "source": [
    "# Store Item Demand Forecasting Challenge - Spark and deep learning\n### link for the github repository (spark part is on ipynb): https://github.com/dimitreOliveira/StoreItemDemand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "a001274bb69489de06b063141c191d87e2a4b02d"
   },
   "outputs": [],
   "source": [
    "# this code get the resulting dataset that i uploaded from my databricks code and commits.\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "submission25 = pd.read_csv('../input/test_data/model25.csv')\n",
    "submission25.to_csv('submission25.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "trusted": true,
    "_uuid": "62b44e0fbab09d6eea683f585f3f32ec712fe0c6",
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
    "\n",
    "days = lambda i: i * 86400\n",
    "get_weekday = udf(lambda x: x.weekday())\n",
    "serie_has_null = F.udf(lambda x: reduce((lambda x, y: x and y), x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "trusted": true,
    "_uuid": "b6b11f1985ee840804bf1ad4a9f1350b93f3e472",
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from keras.models import model_from_json\n",
    "unlist = lambda x: [float(i[0]) for i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "a8522a12f1c56775731ef39442c1a7a6c39bec23",
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    list_result = []\n",
    "    for i in range(len(data)):\n",
    "        list_result.append(np.asarray(data[i]))\n",
    "    return np.asarray(list_result)\n",
    "\n",
    "def prepare_collected_data(data):\n",
    "    list_features = []\n",
    "    list_labels = []\n",
    "    for i in range(len(data)):\n",
    "        list_features.append(np.asarray(data[i][0]))\n",
    "        list_labels.append(data[i][1])\n",
    "    return np.asarray(list_features), np.asarray(list_labels)\n",
    "\n",
    "def prepare_collected_data_test(data):\n",
    "    list_features = []\n",
    "    for i in range(len(data)):\n",
    "        list_features.append(np.asarray(data[i][0]))\n",
    "    return np.asarray(list_features)\n",
    "\n",
    "\n",
    "def save_model(model_path, weights_path, model):\n",
    "    \"\"\"\n",
    "    Save model.\n",
    "    \"\"\"\n",
    "    np.save(weights_path, model.get_weights())\n",
    "    with open(model_path, 'w') as f:\n",
    "        json.dump(model.to_json(), f)\n",
    "    \n",
    "def load_model(model_path, weights_path):\n",
    "    \"\"\"\n",
    "    Load model.\n",
    "    \"\"\"\n",
    "    with open(model_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    model = model_from_json(data)\n",
    "    weights = np.load(weights_path)\n",
    "    model.set_weights(weights)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "trusted": true,
    "_uuid": "80a7ba5be683024abc6313599f40946314de7813",
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "class DateConverter(Transformer):\n",
    "    def __init__(self, inputCol, outputCol):\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "    \n",
    "    def check_input_type(self, schema):\n",
    "        field = schema[self.inputCol]\n",
    "        if (field.dataType != TimestampType()):\n",
    "        raise Exception('Input type %s did not match input type TimestampType' % field.dataType)\n",
    "\n",
    "    def _transform(self, df):\n",
    "        self.check_input_type(df.schema)\n",
    "        return df.withColumn(self.outputCol, df.date.cast(self.inputCol))\n",
    "    \n",
    "    \n",
    "class DayExtractor(Transformer):\n",
    "    def __init__(self, inputCol, outputCol='day'):\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "    \n",
    "    def check_input_type(self, schema):\n",
    "        field = schema[self.inputCol]\n",
    "        if (field.dataType != DateType()):\n",
    "            raise Exception('DayExtractor input type %s did not match input type DateType' % field.dataType)\n",
    "\n",
    "    def _transform(self, df):\n",
    "        self.check_input_type(df.schema)\n",
    "        return df.withColumn(self.outputCol, F.dayofmonth(df[self.inputCol]))\n",
    "    \n",
    "    \n",
    "class MonthExtractor(Transformer):\n",
    "    def __init__(self, inputCol, outputCol='month'):\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "    \n",
    "    def check_input_type(self, schema):\n",
    "        field = schema[self.inputCol]\n",
    "        if (field.dataType != DateType()):\n",
    "            raise Exception('MonthExtractor input type %s did not match input type DateType' % field.dataType)\n",
    "\n",
    "    def _transform(self, df):\n",
    "        self.check_input_type(df.schema)\n",
    "        return df.withColumn(self.outputCol, F.month(df[self.inputCol]))\n",
    "    \n",
    "    \n",
    "class YearExtractor(Transformer):\n",
    "    def __init__(self, inputCol, outputCol='year'):\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "    \n",
    "    def check_input_type(self, schema):\n",
    "        field = schema[self.inputCol]\n",
    "        if (field.dataType != DateType()):\n",
    "            raise Exception('YearExtractor input type %s did not match input type DateType' % field.dataType)\n",
    "\n",
    "    def _transform(self, df):\n",
    "        self.check_input_type(df.schema)\n",
    "        return df.withColumn(self.outputCol, F.year(df[self.inputCol]))\n",
    "    \n",
    "    \n",
    "class WeekDayExtractor(Transformer):\n",
    "    def __init__(self, inputCol, outputCol='weekday'):\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "    \n",
    "    def check_input_type(self, schema):\n",
    "        field = schema[self.inputCol]\n",
    "        if (field.dataType != DateType()):\n",
    "            raise Exception('WeekDayExtractor input type %s did not match input type DateType' % field.dataType)\n",
    "\n",
    "    def _transform(self, df):\n",
    "        self.check_input_type(df.schema)\n",
    "        return df.withColumn(self.outputCol, get_weekday(df[self.inputCol]).cast('int'))\n",
    "    \n",
    "    \n",
    "class WeekendExtractor(Transformer):\n",
    "    def __init__(self, inputCol='weekday', outputCol='weekend'):\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "    \n",
    "    def check_input_type(self, schema):\n",
    "        field = schema[self.inputCol]\n",
    "        if (field.dataType != IntegerType()):\n",
    "            raise Exception('WeekendExtractor input type %s did not match input type IntegerType' % field.dataType)\n",
    "\n",
    "    def _transform(self, df):\n",
    "        self.check_input_type(df.schema)\n",
    "        return df.withColumn(self.outputCol, F.when(((df[self.inputCol] == 5) | (df[self.inputCol] == 6)), 1).otherwise(0))\n",
    "    \n",
    "    \n",
    "class SerieMaker(Transformer):\n",
    "    def __init__(self, inputCol='scaledFeatures', outputCol='serie', dateCol='date', idCol=['store', 'item'], serieSize=30):\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "        self.dateCol = dateCol\n",
    "        self.serieSize = serieSize\n",
    "        self.idCol = idCol\n",
    "\n",
    "    def _transform(self, df):\n",
    "        window = Window.partitionBy(self.idCol).orderBy(self.dateCol)\n",
    "        series = []   \n",
    "        \n",
    "    df = df.withColumn('filled_serie', F.lit(0))\n",
    "    \n",
    "    for index in reversed(range(0, self.serieSize)):\n",
    "        window2 = Window.partitionBy(self.idCol).orderBy(self.dateCol).rowsBetween((30 - index), 30)\n",
    "        col_name = (self.outputCol + '%s' % index)\n",
    "        series.append(col_name)\n",
    "        df = df.withColumn(col_name, F.when(F.isnull(F.lag(F.col(self.inputCol), index).over(window)), F.first(F.col(self.inputCol), ignorenulls=True).over(window2)).otherwise(F.lag(F.col(self.inputCol), index).over(window)))\n",
    "        df = df.withColumn('filled_serie', F.when(F.isnull(F.lag(F.col(self.inputCol), index).over(window)), (F.col('filled_serie') + 1)).otherwise(F.col('filled_serie')))\n",
    "\n",
    "    df = df.withColumn('rank', F.rank().over(window))\n",
    "    df = df.withColumn(self.outputCol, F.array(*series))\n",
    "    \n",
    "    return df.drop(*series)\n",
    "\n",
    "\n",
    "class MonthBeginExtractor(Transformer):\n",
    "    def __init__(self, inputCol='day', outputCol='monthbegin'):\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "    \n",
    "    def check_input_type(self, schema):\n",
    "        field = schema[self.inputCol]\n",
    "        if (field.dataType != IntegerType()):\n",
    "            raise Exception('MonthBeginExtractor input type %s did not match input type IntegerType' % field.dataType)\n",
    "\n",
    "    def _transform(self, df):\n",
    "        self.check_input_type(df.schema)\n",
    "        return df.withColumn(self.outputCol, F.when((df[self.inputCol] <= 7), 1).otherwise(0))\n",
    "    \n",
    "    \n",
    "class MonthEndExtractor(Transformer):\n",
    "    def __init__(self, inputCol='day', outputCol='monthend'):\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "    \n",
    "    def check_input_type(self, schema):\n",
    "        field = schema[self.inputCol]\n",
    "        if (field.dataType != IntegerType()):\n",
    "            raise Exception('MonthEndExtractor input type %s did not match input type IntegerType' % field.dataType)\n",
    "\n",
    "    def _transform(self, df):\n",
    "        self.check_input_type(df.schema)\n",
    "        return df.withColumn(self.outputCol, F.when((df[self.inputCol] >= 24), 1).otherwise(0))\n",
    "    \n",
    "    \n",
    "class YearQuarterExtractor(Transformer):\n",
    "    def __init__(self, inputCol='month', outputCol='yearquarter'):\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "    \n",
    "    def check_input_type(self, schema):\n",
    "        field = schema[self.inputCol]\n",
    "        if (field.dataType != IntegerType()):\n",
    "            raise Exception('YearQuarterExtractor input type %s did not match input type IntegerType' % field.dataType)\n",
    "\n",
    "    def _transform(self, df):\n",
    "        self.check_input_type(df.schema)\n",
    "        return df.withColumn(self.outputCol, F.when((df[self.inputCol] <= 3), 0)\n",
    "                               .otherwise(F.when((df[self.inputCol] <= 6), 1)\n",
    "                                .otherwise(F.when((df[self.inputCol] <= 9), 2)\n",
    "                                 .otherwise(3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "trusted": true,
    "collapsed": true,
    "_uuid": "1493d817e5b0b4bacdbb8f43d56a8814c87b5586",
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "train_data = spark.sql(\"select * from store_item_demand_train_csv\")\n",
    "\n",
    "train, validation = train_data.randomSplit([0.8,0.2], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "trusted": true,
    "collapsed": true,
    "_uuid": "5008c5eaca86134f67524f41d029f9c943633a36",
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# Feature extraction\n",
    "dc = DateConverter(inputCol='date', outputCol='dateFormated')\n",
    "dex = DayExtractor(inputCol='dateFormated')\n",
    "mex = MonthExtractor(inputCol='dateFormated')\n",
    "yex = YearExtractor(inputCol='dateFormated')\n",
    "wdex = WeekDayExtractor(inputCol='dateFormated')\n",
    "wex = WeekendExtractor()\n",
    "mbex = MonthBeginExtractor()\n",
    "meex = MonthEndExtractor()\n",
    "yqex = YearQuarterExtractor()\n",
    "\n",
    "# Data process\n",
    "va = VectorAssembler(inputCols=['store', 'item', 'day', 'month', 'year', 'weekday', 'weekend', 'monthbegin', 'monthend', 'yearquarter'], outputCol=\"features\")\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "# Serialize data\n",
    "sm = SerieMaker(inputCol='scaledFeatures', dateCol='date', idCol=['store', 'item'], serieSize=15)\n",
    "\n",
    "pipeline = Pipeline(stages=[dc, dex, mex, yex, wdex, wex, mbex, meex, yqex, va, scaler, sm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "trusted": true,
    "collapsed": true,
    "_uuid": "a3a36de54795c09ea5906187ad798e52659b807d",
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "pipiline_model = pipeline.fit(train)\n",
    "\n",
    "train_transformed = pipiline_model.transform(train)\n",
    "validation_transformed = pipiline_model.transform(validation)\n",
    "\n",
    "train_transformed.write.saveAsTable('train_transformed_15', mode='overwrite')\n",
    "validation_transformed.write.saveAsTable('validation_transformed_15', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "trusted": true,
    "collapsed": true,
    "_uuid": "92094494a7e582a25c8cb93bbbdf582bf626555e",
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "test_data = spark.sql(\"select * from store_item_demand_test_csv\")\n",
    "test_transformed = pipiline_model.transform(test_data)\n",
    "test_transformed.write.saveAsTable('test_transformed_15', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "trusted": true,
    "collapsed": true,
    "_uuid": "3356d0653bb621d4a52456910d71212493792b2c",
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, GRU\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "train_transformed = spark.sql(\"select * from train_transformed\")\n",
    "validation_transformed = spark.sql(\"select * from validation_transformed\")\n",
    "\n",
    "train_x, train_y = prepare_collected_data(train_transformed.select('serie', 'sales').collect())\n",
    "validation_x, validation_y = prepare_collected_data(validation_transformed.select('serie', 'sales').collect())\n",
    "\n",
    "n_label = 1\n",
    "serie_size = len(train_x[0])\n",
    "n_features = len(train_x[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "trusted": true,
    "collapsed": true,
    "_uuid": "c7cada1499116151945e8e24099ecec12ca18b16",
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "epochs = 80\n",
    "batch = 512\n",
    "lr = 0.001\n",
    "\n",
    "# design network\n",
    "model = Sequential()\n",
    "model.add(GRU(40, input_shape=(serie_size, n_features)))\n",
    "model.add(Dense(10, kernel_initializer='glorot_normal', activation='relu'))\n",
    "model.add(Dense(n_label))\n",
    "model.summary()\n",
    "\n",
    "adam = optimizers.Adam(lr)\n",
    "model.compile(loss='mae', optimizer=adam, metrics=['mse', 'msle'])\n",
    "\n",
    "history = model.fit(train_x, train_y, epochs=epochs, batch_size=batch, validation_data=(validation_x, validation_y), verbose=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "trusted": true,
    "collapsed": true,
    "_uuid": "29720d4fab8b02adc26ae8e9776b324b0a2a6c30",
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "model_path = '/dbfs/user/model1.json'\n",
    "weights_path = '/dbfs/user/weights1.npy'\n",
    "save_model(model_path, weights_path, model)\n",
    "\n",
    "predictions = model.predict(validation_x)\n",
    "\n",
    "import pandas as pd\n",
    "ids = validation_y\n",
    "df = pd.DataFrame(ids, columns=['label'])\n",
    "df['sales'] = predictions\n",
    "df_predictions = spark.createDataFrame(df)\n",
    "\n",
    "rmse_evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"sales\", metricName=\"rmse\")\n",
    "mse_evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"sales\", metricName=\"mse\")\n",
    "mae_evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"sales\", metricName=\"mae\")\n",
    "\n",
    "validation_rmse = rmse_evaluator.evaluate(df_predictions)\n",
    "validation_mse = mse_evaluator.evaluate(df_predictions)\n",
    "validation_mae = mae_evaluator.evaluate(df_predictions)\n",
    "print(\"RMSE: %f, MSE: %f, MAE: %f\" % (validation_rmse, validation_mse, validation_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "trusted": true,
    "collapsed": true,
    "_uuid": "e63e77e84fe3fda9c3052d542abd854ef6b59b51",
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "model_path = '/dbfs/user/model1.json'\n",
    "weights_path = '/dbfs/user/weights1.npy'\n",
    "model = load_model(model_path, weights_path)\n",
    "\n",
    "test_transformed = spark.sql(\"select * from test_transformed_15\")\n",
    "\n",
    "test = prepare_collected_data_test(test_transformed.select('serie').collect())\n",
    "\n",
    "ids = test_transformed.select('id').collect()\n",
    "\n",
    "predictions = model.predict(test)\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(ids, columns=['id'])\n",
    "df['sales'] = predictions\n",
    "df_predictions = spark.createDataFrame(df)\n",
    "\n",
    "df_predictions = df_predictions.withColumn('sales', df_predictions['sales'].cast('int'))\n",
    "display(df_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
