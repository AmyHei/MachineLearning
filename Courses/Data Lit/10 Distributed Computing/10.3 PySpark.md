## PySpark

##### SPARK – What is it?  

A fast and general compute engine for Hadoop data. Spark provides a simple and expressive programming model that supports a wide range of applications, including ETL, machine learning, stream processing, and graph computation.  

To Learn more, follow this link: https://mapr.com/blog/spark-101-what-it-what-it-does-and-why-it-matters/ 

Why spark and how it is different from Hadoop?  

Let’s say we want to travel from Lisbon, Portugal to Shanghai, China. There are 3 ways we can reach the destination. 1. By Waterways – Straight forward but will take months to reach. 2. Roadways – 6 days of continuous travel plus need to cross at least 7 countries with all visa and other regulations. 3. Airways – 17 hours max and you can reach without much hassle. Here 1 refers to break big data without distributed computing. 2 refers to Hadoop with complications and huge data handling in disk memory. Number 3 refers to the spark fast and safe in memory distributed computing.  

Although critics of Spark’s in-memory processing admit that Spark is very fast (Up to 100 times faster than Hadoop MapReduce), they might not be so ready to acknowledge that it runs up to ten times faster on disk. Spark can also perform batch processing; however, it really excels at streaming workloads, interactive queries, and machine-based learning. 

Spark’s big claim to fame is its real-time data processing capability as compared to MapReduce’s disk-bound, batch processing engine. Spark is compatible with Hadoop and its modules. In fact, on Hadoop’s project page, Spark is listed as a module. 

Spark has its own page because, while it can run in Hadoop clusters through YARN (Yet Another Resource Negotiator), it also has a standalone mode. The fact that it can run as a Hadoop module and as a standalone solution makes it tricky to directly compare and contrast. However, as time goes on, some big data scientists expect Spark to diverge and perhaps replace Hadoop, especially in instances where faster access to processed data is critical. 

Spark is a cluster-computing framework, which means that it competes more with MapReduce than with the entire Hadoop ecosystem. For example, Spark doesn’t have its own distributed filesystem but can use HDFS. 

Spark uses memory and can use the disk for processing, whereas MapReduce is strictly disk-based. The primary difference between MapReduce and Spark is that MapReduce uses persistent storage and Spark uses Resilient Distributed Datasets (RDDs), which is covered in more detail under the Fault Tolerance section. 

Conclusion: Learn SPARK it will Definity help you. https://www.tutorialspoint.com/pyspark/index.htm 

