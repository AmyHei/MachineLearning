### Hierarchical Clustering

- In this blog, I mainly cited
- https://stackabuse.com/hierarchical-clustering-with-python-and-scikit-learn/

![](https://www.theschool.ai/wp-content/uploads/2019/03/hierarchical.png)

## Introduction

#### Hierarchical clustering

- It is a type of **unsupervised machine learning** algorithm used to cluster **unlabeled data points**.
- It **groups together** the data points with similar characteristics *like K-means clustering**.

#### Two Types of hierarchical clustering
1. **Agglomerative** :  Data points are clustered using a bottom-up approach starting with individual data points
2. **Divisive** :  **top-down approach** is followed where all the data points are treated as one big cluster and the clustering process involves dividing the one big cluster into several small clusters.

#### 5 Steps to perform hierarchical clustering
1. Treat each data point as one cluster. (The number of clusters will be K, an integer representing the number of data points.)
2. Form a cluster by joining the two closest data points resulting in K-1 clusters.
3. Form more clusters by joining the two closest clusters resulting in K-2 clusters.
4. Repeat the above three steps until one big cluster is formed.
5. Once single cluster is formed, dendrograms are used to divide into multiple clusters depending upon the problem.
 
![](https://www.theschool.ai/wp-content/uploads/2019/03/clustering.jpg)

## Before start

You can try this here
- [Colab](https://www.theschool.ai/wp-content/uploads/2019/03/colab.png)
- Github https://github.com/decoderkurt/DataLit_week8_hierarchical_clustering

## Code

### Example 1

#### 1. Dependencies
```
import matplotlib.pyplot as plt  
import pandas as pd  
%matplotlib inline
import numpy as np 
```

#### 2. Create the dataset
```
X = np.array([[5,3],  
    [10,15],
    [15,12],
    [24,10],
    [30,30],
    [85,70],
    [71,80],
    [60,78],
    [70,55],
    [80,91],])
```

#### 3. Call clustering function using Scikit-Learn
```
from sklearn.cluster import AgglomerativeClustering

cluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')  
cluster.fit_predict(X)
```

1. **AgglomerativeClustering**
- the affinity set to “euclidean” means distance between the datapoints.
- linkage parameter set to “ward” minimizes the variant between the clusters

2. **fit_predict**
- It returns the names of the clusters that each data point belongs to.

#### 4. See how the data points have been clustered
```
print(cluster.labels_)
```
```
output : [1 1 1 1 1 0 0 0 0]
one-dimensional array of 10 elements corresponding to the clusters assigned to our 10 data points.
```

#### 5. Dendrogram
```
import scipy.cluster.hierarchy as shc

plt.figure(figsize=(10, 7))  
plt.title("Example Dendograms")  
dend = shc.dendrogram(shc.linkage(X, method='ward'))
```

The output graph looks like the one below.

![](https://www.theschool.ai/wp-content/uploads/2019/03/dend1.png)

- It starts by finding the two points that are closest to each other on the basis of Euclidean distance.
- Points 1 and 2 are closest to each other while points 6 and 7 are closes to each other.
- Clustering will be formed between these two closest points first.

#### Dendograms

- The vertical height shows the Euclidean distances between points.
- From the above graph, the Euclidean distance between points 6 and 7 is greater than the distance between point 1 and 2.
- The next step is to join the cluster formed by joining two points to the next nearest cluster or point which in turn results in another cluster.
- Point 3 is closest to cluster of point 1 and 2, therefore dendrogram is generated by joining point 3 with dendrogram of point 1 and 2.
- This process continues until all the points are joined together to form one big cluster.
- Once one big cluster is formed, the longest vertical distance without any horizontal line passing through it is selected and a horizontal line is drawn through it.
- The number of vertical lines this newly created horizontal line passes is equal to number of clusters.

![](https://www.theschool.ai/wp-content/uploads/2019/03/dend1-2.png)

- The largest vertical distance without any horizontal line passing through it is represented by blue line.
- A new horizontal red line that passes through the blue line crosses the blue line at two points, therefore the number of clusters will be 2.
- **The horizontal line is a threshold**, which defines the minimum distance required to be a separate cluster.

![](https://www.theschool.ai/wp-content/uploads/2019/03/dend1-1.png)

- In the above plot, the red horizontal line passes through four vertical lines resulting in four clusters.
- Point 4, cluster of points 1,2,3 and 0, cluster of points 5 and 8, cluster of  points 6,7 and 9.

#### 5. Visualize it
```
plt.scatter(X[:,0],X[:,1], c=cluster.labels_, cmap='rainbow')
```

![](https://www.theschool.ai/wp-content/uploads/2019/03/hierarchical-clustering-python-scikit-learn-5.png)

### Example 2

#### 1. Dependencies
```
import matplotlib.pyplot as plt  
import pandas as pd  
%matplotlib inline
import numpy as np  
```

#### 2. Load the dataset
```
url = "https://raw.githubusercontent.com/decoderkurt/DataLit_week8_hierarchical_clustering/master/shopping_data.csv"
customer_data = pd.read_csv(url)
```

#### 3. Take a look at the dataset 
```
customer_data.shape
```
```
output: (200, 5)
```

#### 4. Dependencies
```
customer_data.head()
```

![](https://www.theschool.ai/wp-content/uploads/2019/03/dataset.png)

```
data = customer_data.iloc[:, 3:5].values
```

#### 5. Dendrogram
```
import scipy.cluster.hierarchy as shc

plt.figure(figsize=(10, 7))  
plt.title("Customer Dendograms")  
dend = shc.dendrogram(shc.linkage(data, method='ward'))
```

![](https://www.theschool.ai/wp-content/uploads/2019/03/dend2.png)

#### 5.  Call  fit_predict using Scikit-Learn
```
from sklearn.cluster import AgglomerativeClustering

cluster = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')  
cluster.fit_predict(data)  
```

#### 6.  Visualize it
```
plt.figure(figsize=(10, 7))  
plt.scatter(data[:,0], data[:,1], c=cluster.labels_, cmap='rainbow')
```

![](https://www.theschool.ai/wp-content/uploads/2019/03/ex2.png)

### Conclusion

We studied the concept of hierarchical clustering and implemented it by Python.

- Scikit-Learn is very useful to implement hierarchical clustering.
- Remember how to use both AgglomerativeClustering class and fit_predict function.
- Remember how to read Dendrogram plot.

## Further reading

- https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial
- [Data Science in Python, Pandas, Scikit-learn, Numpy, Matplotlib](http://stackabu.se/data-science-python-pandas-sklearn-numpy)
- [Python for Data Science and Machine Learning Bootcamp](http://stackabu.se/python-data-science-machine-learning-bootcamp)
- [Machine Learning A-Z: Hands-On Python & R In Data Science](http://stackabu.se/machine-learning-hands-on-python-data-science)